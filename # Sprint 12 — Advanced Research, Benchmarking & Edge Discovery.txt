# Sprint 12 ‚Äî Advanced Research, Benchmarking & Edge Discovery

## 1) Research Data Pipeline

**Goal:** make historical data easily queryable for analysis.

**Features**

* Export journals + snapshots into structured DB (SQLite / Timescale).
* Schema aligns with `snapshot@1.3` & `journal@1.3`.
* Provide scripts/tools for:

  * Trade chains (setup ‚Üí fill ‚Üí exit).
  * Edge case activations with outcome stats.
  * Confluence scores vs results.
* Standard CSV/Parquet exports for ML pipelines.

---

## 2) Strategy Benchmarking Framework

**Goal:** compare ‚Äúwhat if‚Äù playbooks against live/journal data.

**Features**

* Define **alt-strategies**: e.g.,

  * ‚ÄúSwing only when confluence ‚â•0.9‚Äù
  * ‚ÄúScalp only in London session‚Äù
  * ‚ÄúExclude Edge Cases family C‚Äù
* Backtest alt-strategies over archived journals.
* Report KPIs (win rate, expectancy, DD, payoff, Sharpe-like).
* UI Benchmark tab: select strategy filters ‚Üí compare curves side by side.

---

## 3) Machine-Learning Integrations

**Goal:** allow ML to explore confluence patterns & predict outcomes.

**Features**

* Export ML-ready features:

  * Component scores per trade.
  * Patterns (candles/harmonics).
  * Edge case family activations.
  * Session & news context.
* Labels: outcome R, MAE/MFE, exit reason.
* Provide Python notebooks (Jupyter) with baseline ML workflows:

  * Logistic regression ‚Üí win probability.
  * Random forest / gradient boosting ‚Üí feature importances.
  * Neural nets (optional) for sequential confluence patterns.

---

## 4) Automated Edge Discovery

**Goal:** surface recurring conditions that precede profitable setups.

**Features**

* Statistical scans: ‚ÄúWhen confluence.momentum >0.7 AND Edge Case=Liq Sweep ‚Üí win rate 68%, avg R=1.3.‚Äù
* Cluster analysis on patterns: group trades by similarity, identify high-performing clusters.
* Surfaced edges flagged into UI as **Hypotheses**.
* Stored in `/docs/research/edges.json`.

---

## 5) Research Dashboard (UI)

* New tab: **Research & Edges**

  * Trade cluster explorer (scatterplot of confluence vs R).
  * Feature importance panel (ML results).
  * Edge hypotheses list (with emoji + conditions).
  * ‚ÄúBacktest Alt-Strategy‚Äù controls ‚Üí run filter & see metrics.

---

## 6) Governance for ML

* **Manual approval required**: EA never auto-trades from ML suggestions.
* **All hypotheses logged**: journal `event:"research_hypothesis"` with score + rationale.
* **Transparency first**: ML outputs treated as **advisory**, not as automation.

---

# üö¶ Succinct Sprint-12 Checklist

**Research Data**

* [ ] Journals & snapshots export to DB/CSV/Parquet
* [ ] Query scripts for trades, edges, confluence outcomes
* [ ] Schema matches snapshot\@1.3/journal\@1.3

**Benchmarking**

* [ ] Alt-strategy filters configurable
* [ ] Backtest engine runs filters on archive
* [ ] KPIs computed (win%, expectancy, DD, payoff, Sharpe)
* [ ] UI Benchmark tab with side-by-side curves

**ML Integrations**

* [ ] Feature export (confluence, patterns, edge cases, sessions)
* [ ] Label export (R, MAE/MFE, exit reason)
* [ ] Jupyter notebooks with baseline ML pipelines
* [ ] Results export back to UI

**Edge Discovery**

* [ ] Statistical scans for condition‚Üíoutcome
* [ ] Cluster analysis of trades
* [ ] Hypotheses emitted to edges.json
* [ ] UI shows hypotheses tab

**Governance**

* [ ] ML suggestions advisory only
* [ ] Hypotheses logged in journal
* [ ] Operator review required

**Acceptance**

* [ ] Archive‚ÜíDB export reproducible
* [ ] Benchmarking results align with journals
* [ ] ML pipelines run out-of-box on exported data
* [ ] UI surfaces hypotheses clearly
* [ ] No automation from ML without operator

---